{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Load and preprocess the data"
      ],
      "metadata": {
        "id": "IExSSu4OwfsA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKFIZ6Irvmt3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import datetime\n",
        "\n",
        "import IPython\n",
        "import IPython.display\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/mnt/data/dataSet LoraWAN.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Convert time column to datetime\n",
        "df['time'] = pd.to_datetime(df['time'])\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "df.head()\n",
        "\n",
        "# Check for missing values\n",
        "print(df.isna().sum())\n",
        "\n",
        "# Fill missing values in 'power_consumption' with the mean value of the column\n",
        "df['power_consumption'].fillna(df['power_consumption'].mean(), inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Resample the data"
      ],
      "metadata": {
        "id": "-zuY2snMyqPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Resample the data to hourly intervals\n",
        "df.set_index('time', inplace=True)\n",
        "df = df.resample('H').mean()\n",
        "\n",
        "# Reset the index\n",
        "df.reset_index(inplace=True)\n",
        "\n",
        "# Display the first few rows of the resampled dataset\n",
        "df.head()"
      ],
      "metadata": {
        "id": "7-OHShHhzCKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Split the data"
      ],
      "metadata": {
        "id": "gxABkh3UzKCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the split ratios\n",
        "train_ratio = 0.7\n",
        "val_ratio = 0.2\n",
        "test_ratio = 0.1\n",
        "\n",
        "# Split the data\n",
        "n = len(df)\n",
        "train_df = df[:int(n * train_ratio)]\n",
        "val_df = df[int(n * train_ratio):int(n * (train_ratio + val_ratio))]\n",
        "test_df = df[int(n * (train_ratio + val_ratio)):]\n",
        "\n",
        "# Display the sizes of the splits\n",
        "print(len(train_df), len(val_df), len(test_df))"
      ],
      "metadata": {
        "id": "8vF1BieEzMV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Normalize the data"
      ],
      "metadata": {
        "id": "teo_hGhYzR2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the data\n",
        "train_mean = train_df.mean()\n",
        "train_std = train_df.std()\n",
        "\n",
        "train_df = (train_df - train_mean) / train_std\n",
        "val_df = (val_df - train_mean) / train_std\n",
        "test_df = (test_df - train_mean) / train_std"
      ],
      "metadata": {
        "id": "fJOGs-SHzVWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Create data windows"
      ],
      "metadata": {
        "id": "Wlu6uMa8zZZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "class WindowGenerator():\n",
        "    def __init__(self, input_width, label_width, shift, train_df, val_df, test_df, label_columns=None):\n",
        "        # Store the raw data.\n",
        "        self.train_df = train_df\n",
        "        self.val_df = val_df\n",
        "        self.test_df = test_df\n",
        "\n",
        "        # Work out the label column indices.\n",
        "        self.label_columns = label_columns\n",
        "        if label_columns is not None:\n",
        "            self.label_columns_indices = {name: i for i, name in enumerate(label_columns)}\n",
        "        self.column_indices = {name: i for i, name in enumerate(train_df.columns)}\n",
        "\n",
        "        # Work out the window parameters.\n",
        "        self.input_width = input_width\n",
        "        self.label_width = label_width\n",
        "        self.shift = shift\n",
        "\n",
        "        self.total_window_size = input_width + shift\n",
        "\n",
        "        self.input_slice = slice(0, input_width)\n",
        "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
        "\n",
        "        self.label_start = self.total_window_size - self.label_width\n",
        "        self.labels_slice = slice(self.label_start, None)\n",
        "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '\\n'.join([\n",
        "            f'Total window size: {self.total_window_size}',\n",
        "            f'Input indices: {self.input_indices}',\n",
        "            f'Label indices: {self.label_indices}',\n",
        "            f'Label column name(s): {self.label_columns}'\n",
        "        ])\n",
        "\n",
        "    def split_window(self, features):\n",
        "        inputs = features[:, self.input_slice, :]\n",
        "        labels = features[:, self.labels_slice, :]\n",
        "        if self.label_columns is not None:\n",
        "            labels = tf.stack(\n",
        "                [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
        "                axis=-1)\n",
        "\n",
        "        # Slicing doesn't preserve static shape information, so set the shapes\n",
        "        # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
        "        inputs.set_shape([None, self.input_width, None])\n",
        "        labels.set_shape([None, self.label_width, None])\n",
        "\n",
        "        return inputs, labels\n",
        "\n",
        "    def plot(self, model=None, plot_col='power_consumption', max_subplots=3):\n",
        "        inputs, labels = self.example\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plot_col_index = self.column_indices[plot_col]\n",
        "        max_n = min(max_subplots, len(inputs))\n",
        "        for n in range(max_n):\n",
        "            plt.subplot(max_n, 1, n + 1)\n",
        "            plt.ylabel(f'{plot_col} [normed]')\n",
        "            plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n",
        "                     label='Inputs', marker='.', zorder=-10)\n",
        "            if self.label_columns:\n",
        "                label_col_index = self.label_columns_indices.get(plot_col, None)\n",
        "            else:\n",
        "                label_col_index = plot_col_index\n",
        "\n",
        "            if label_col_index is None:\n",
        "                continue\n",
        "\n",
        "            plt.scatter(self.label_indices, labels[n, :, label_col_index],\n",
        "                        edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
        "            if model is not None:\n",
        "                predictions = model(inputs)\n",
        "                plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n",
        "                            marker='X', edgecolors='k', label='Predictions',\n",
        "                            c='#ff7f0e', s=64)\n",
        "\n",
        "            if n == 0:\n",
        "                plt.legend()\n",
        "\n",
        "        plt.xlabel('Time [h]')\n",
        "\n",
        "    def make_dataset(self, data):\n",
        "        data = np.array(data, dtype=np.float32)\n",
        "        ds = tf.keras.utils.timeseries_dataset_from_array(\n",
        "            data=data,\n",
        "            targets=None,\n",
        "            sequence_length=self.total_window_size,\n",
        "            sequence_stride=1,\n",
        "            shuffle=True,\n",
        "            batch_size=32)\n",
        "\n",
        "        ds = ds.map(self.split_window)\n",
        "\n",
        "        return ds\n",
        "\n",
        "    @property\n",
        "    def train(self):\n",
        "        return self.make_dataset(self.train_df)\n",
        "\n",
        "    @property\n",
        "    def val(self):\n",
        "        return self.make_dataset(self.val_df)\n",
        "\n",
        "    @property\n",
        "    def test(self):\n",
        "        return self.make_dataset(self.test_df)\n",
        "\n",
        "    @property\n",
        "    def example(self):\n",
        "        \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
        "        result = getattr(self, '_example', None)\n",
        "        if result is None:\n",
        "            # No example batch was found, so get one from the `.train` dataset\n",
        "            result = next(iter(self.train))\n",
        "            # And cache it for next time\n",
        "            self._example = result\n",
        "        return result\n",
        "\n",
        "# Create WindowGenerator object\n",
        "input_width = 24\n",
        "label_width = 1\n",
        "shift = 24\n",
        "label_columns = ['power_consumption']\n",
        "w1 = WindowGenerator(input_width, label_width, shift, train_df, val_df, test_df, label_columns)\n",
        "\n",
        "# Display window configuration\n",
        "print(w1)\n"
      ],
      "metadata": {
        "id": "WdQL-g1-zcpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Build and train the model"
      ],
      "metadata": {
        "id": "MUiCQZbNzgfv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "\n",
        "# Define the model\n",
        "model = Sequential([\n",
        "    LSTM(32, return_sequences=False),\n",
        "    Dense(units=1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(w1.train, epochs=10, validation_data=w1.val)\n"
      ],
      "metadata": {
        "id": "FWjhnDOazlb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: Evaluate the model"
      ],
      "metadata": {
        "id": "aIZHZa8oznzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "val_performance = model.evaluate(w1.val)\n",
        "test_performance = model.evaluate(w1.test)\n",
        "\n",
        "print(f'Validation MSE: {val_performance}')\n",
        "print(f'Test MSE: {test_performance}')\n"
      ],
      "metadata": {
        "id": "BMbgdruVzrym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 8: Plot the results"
      ],
      "metadata": {
        "id": "jw-tEM1jzumg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the predictions\n",
        "w1.plot(model=model)\n"
      ],
      "metadata": {
        "id": "FEeUwwYCzx0L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}